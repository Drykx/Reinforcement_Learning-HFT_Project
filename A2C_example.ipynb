{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Another Example"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import external modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gymnasium as gym\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import itertools\n",
    "\n",
    "from stable_baselines3 import PPO, A2C\n",
    "from stable_baselines3.common.callbacks import EvalCallback\n",
    "from stable_baselines3.common.env_util import make_vec_env\n",
    "from stable_baselines3.common.vec_env import VecMonitor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Add mbt-gym to path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append(\"../\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mbt_gym.agents.BaselineAgents import CarteaJaimungalMmAgent\n",
    "from mbt_gym.gym.helpers.generate_trajectory import generate_trajectory\n",
    "from mbt_gym.gym.StableBaselinesTradingEnvironment import StableBaselinesTradingEnvironment\n",
    "from mbt_gym.gym.TradingEnvironment import TradingEnvironment\n",
    "from mbt_gym.gym.wrappers import *\n",
    "from mbt_gym.rewards.RewardFunctions import PnL, CjMmCriterion, RunningInventoryPenalty\n",
    "from mbt_gym.stochastic_processes.midprice_models import BrownianMotionMidpriceModel, BrownianMotionJumpMidpriceModel\n",
    "from mbt_gym.stochastic_processes.arrival_models import PoissonArrivalModel, HawkesArrivalModel\n",
    "from mbt_gym.stochastic_processes.fill_probability_models import ExponentialFillFunction\n",
    "from mbt_gym.gym.ModelDynamics import LimitOrderModelDynamics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create market making environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "terminal_time = 1.0\n",
    "arrival_rate = 10.0\n",
    "n_steps = int(10 * terminal_time * arrival_rate)\n",
    "phi = 0.5\n",
    "alpha = 0.001"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get the environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_new_env(num_trajectories:int = 1):\n",
    "    fill_exponent = 1\n",
    "    sigma = 0.1\n",
    "    initial_inventory = (-4,5)\n",
    "    initial_price = 100\n",
    "    step_size = 1/n_steps\n",
    "    timestamps = np.linspace(0, terminal_time, n_steps + 1)\n",
    "    midprice_model = BrownianMotionJumpMidpriceModel(volatility=sigma, step_size=1/n_steps, jump_size = 1,\n",
    "                                                 num_trajectories=num_trajectories)\n",
    "    arrival_model = HawkesArrivalModel(step_size=1/n_steps, \n",
    "                                        num_trajectories=num_trajectories)\n",
    "    fill_probability_model = ExponentialFillFunction(fill_exponent=fill_exponent, \n",
    "                                                     step_size=1/n_steps,\n",
    "                                                     num_trajectories=num_trajectories)\n",
    "    LOtrader = LimitOrderModelDynamics(midprice_model = midprice_model, arrival_model = arrival_model, \n",
    "                                fill_probability_model = fill_probability_model,\n",
    "                                num_trajectories = num_trajectories)\n",
    "    reward_function = RunningInventoryPenalty(per_step_inventory_aversion = phi, terminal_inventory_aversion = alpha)\n",
    "    env_params = dict(terminal_time=terminal_time, \n",
    "                      n_steps=n_steps,\n",
    "                      initial_inventory = initial_inventory,\n",
    "                      model_dynamics = LOtrader,\n",
    "                      max_inventory=n_steps,\n",
    "                      normalise_action_space = False,\n",
    "                      normalise_observation_space = False,\n",
    "                      reward_function = reward_function,\n",
    "                      num_trajectories=num_trajectories)\n",
    "    return TradingEnvironment(**env_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_new_env(\n",
    "    drift=0.0, \n",
    "    volatility=0.1, \n",
    "    jump_size=1.0, \n",
    "    initial_price=100, \n",
    "    step_size=1/n_steps, \n",
    "    num_trajectories=1, \n",
    "    initial_inventory = (-4,5),\n",
    "    inventory_exponent=2.0, \n",
    "    baseline_arrival_rate=np.array([[10.0, 10.0]]), \n",
    "    arrival_jump_size=40.0, \n",
    "    mean_reversion_speed=60.0,\n",
    "    timestamps = np.linspace(0, terminal_time, n_steps + 1)\n",
    "):\n",
    "    n_steps = int(terminal_time / step_size)\n",
    "    \n",
    "    # Simulate midprice using Brownian motion with jumps\n",
    "    midprice_model = BrownianMotionJumpMidpriceModel(\n",
    "        drift=drift, \n",
    "        volatility=volatility, \n",
    "        jump_size=jump_size, \n",
    "        initial_price=initial_price, \n",
    "        terminal_time=terminal_time, \n",
    "        step_size=step_size, \n",
    "        num_trajectories=num_trajectories\n",
    "    )\n",
    "    \n",
    "    # Simulate order arrivals using Hawkes process\n",
    "    arrival_model = HawkesArrivalModel(\n",
    "        baseline_arrival_rate=baseline_arrival_rate, \n",
    "        step_size=step_size, \n",
    "        jump_size=arrival_jump_size, \n",
    "        mean_reversion_speed=mean_reversion_speed, \n",
    "        terminal_time=terminal_time, \n",
    "        num_trajectories=num_trajectories\n",
    "    )\n",
    "    \n",
    "    # Simulate fill probability for limit orders\n",
    "    fill_probability_model = ExponentialFillFunction(\n",
    "        fill_exponent=1, \n",
    "        step_size=step_size, \n",
    "        num_trajectories=num_trajectories\n",
    "    )\n",
    "    \n",
    "    # Combine models to create market dynamics\n",
    "    LOtrader = LimitOrderModelDynamics(\n",
    "        midprice_model=midprice_model, \n",
    "        arrival_model=arrival_model, \n",
    "        fill_probability_model=fill_probability_model, \n",
    "        num_trajectories=num_trajectories\n",
    "    )\n",
    "    \n",
    "    # Define the reward function based on inventory levels\n",
    "    reward_function = RunningInventoryPenalty(\n",
    "        per_step_inventory_aversion=phi, # TO TEST\n",
    "        terminal_inventory_aversion=alpha, # TO TEST\n",
    "        inventory_exponent=inventory_exponent # TO TEST\n",
    "    )\n",
    "    \n",
    "    # Create the trading environment\n",
    "    env_params = dict(\n",
    "        terminal_time=terminal_time, \n",
    "        n_steps=n_steps, \n",
    "        initial_inventory=initial_inventory, \n",
    "        model_dynamics=LOtrader, \n",
    "        max_inventory=100, \n",
    "        normalise_action_space=False, # TO CHECK\n",
    "        normalise_observation_space=False, # TO CHECK\n",
    "        reward_function=reward_function, \n",
    "        num_trajectories=num_trajectories\n",
    "    )\n",
    "    \n",
    "    return TradingEnvironment(**env_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_trajectories = 1000\n",
    "env = ReduceStateSizeWrapper(get_new_env(num_trajectories))\n",
    "sb_env = StableBaselinesTradingEnvironment(trading_env=env)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Monitor sb_env\n",
    "sb_env = VecMonitor(sb_env)\n",
    "# Add directory for tensorboard logging and best model\n",
    "tensorboard_logdir = \"./tensorboard/PPO-learning-CJ/\"\n",
    "best_model_path = \"./SB_models/PPO-best-CJ\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define the Policy: Robust PPO "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The simplicity and synchronous nature of A2C might be advantageous in high-frequency trading scenarios where decisions need to be made quickly. It is more suitable in dynamic trading environments."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Define lists of values for each significant A2C parameter\n",
    "learning_rate_list = [0.0001, 0.001, 0.01]\n",
    "n_steps_list = [5, 10, 20]\n",
    "gamma_list = [0.95, 0.99]\n",
    "gae_lambda_list = [0.9, 0.95, 1.0]\n",
    "ent_coef_list = [0.01, 0.05, 0.1]\n",
    "vf_coef_list = [0.25, 0.5, 0.75]\n",
    "normalize_advantage_list = [True, False]\n",
    "\n",
    "# Define the policy network architecture\n",
    "policy_kwargs = dict(net_arch=[dict(pi=[256, 256], vf=[256, 256])])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "callback_params = dict(eval_env=sb_env, n_eval_episodes = 2048, \n",
    "                        best_model_save_path = best_model_path, \n",
    "                        deterministic=True)\n",
    "\n",
    "callback = EvalCallback(**callback_params)\n",
    "\n",
    "# Iterate over all combinations of A2C parameter values\n",
    "for learning_rate, n_steps, gamma, gae_lambda, ent_coef, vf_coef, normalize_advantage in itertools.product(\n",
    "        learning_rate_list, n_steps_list, gamma_list, gae_lambda_list, ent_coef_list, vf_coef_list, normalize_advantage_list):\n",
    "\n",
    "        # Print current combination of parameters\n",
    "        print(f\"Training with: learning_rate={learning_rate}, n_steps={n_steps}, gamma={gamma}, gae_lambda={gae_lambda}, \"\n",
    "                f\"ent_coef={ent_coef}, vf_coef={vf_coef}, normalize_advantage={normalize_advantage}\")\n",
    "\n",
    "        A2C_params = {\"policy\":'MlpPolicy', \"env\": sb_env, \"verbose\":1, \n",
    "                \"policy_kwargs\":policy_kwargs, \n",
    "                \"tensorboard_log\":tensorboard_logdir,\n",
    "                \"learning_rate\": learning_rate, \n",
    "                \"gamma\":gamma, \n",
    "                \"gae_lambda\":gae_lambda, \n",
    "                \"ent_coef\":ent_coef, \n",
    "                \"vf_coef\":vf_coef, \n",
    "                \"max_grad_norm\":0.5, \n",
    "                \"rms_prop_eps\":1e-5, \n",
    "                \"n_steps\": int(n_steps),\n",
    "                \"normalize_advantage\":normalize_advantage\n",
    "                }\n",
    "\n",
    "        # Initialize the A2C model with the current set of parameters\n",
    "        model = A2C(**A2C_params,device=\"cpu\")\n",
    "        \n",
    "        # Train the model\n",
    "        model.learn(total_timesteps=10_000_000)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define the Agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mbt_gym.agents.SbAgent import SbAgent\n",
    "A2C_agent = SbAgent(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class HistoricalDataEnv(gym.Env):\n",
    "    def __init__(self, data: pd.DataFrame, initial_inventory=0, max_inventory=100, window_size=10):\n",
    "        super(HistoricalDataEnv, self).__init__()\n",
    "        self.data = data\n",
    "        self.initial_inventory = initial_inventory\n",
    "        self.max_inventory = max_inventory\n",
    "        self.window_size = window_size\n",
    "        self.current_step = 0\n",
    "        self.inventory = initial_inventory\n",
    "        \n",
    "        # Define action and observation space\n",
    "        self.action_space = gym.spaces.Discrete(3)  # 0: Hold, 1: Bid, 2: Ask\n",
    "        self.observation_space = gym.spaces.Box(low=0, high=1, shape=(window_size, len(data.columns)), dtype=np.float32)\n",
    "        \n",
    "    def reset(self):\n",
    "        self.current_step = 0\n",
    "        self.inventory = self.initial_inventory\n",
    "        return self._next_observation()\n",
    "    \n",
    "    def _next_observation(self):\n",
    "        obs = self.data.iloc[self.current_step:self.current_step + self.window_size].values\n",
    "        obs = np.append(obs, [[self.inventory] * len(self.data.columns)], axis=0)\n",
    "        return obs\n",
    "    \n",
    "    def step(self, action):\n",
    "        current_price = self.data.iloc[self.current_step]['mid_price']\n",
    "        reward = 0\n",
    "        \n",
    "        if action == 1:  # Buy\n",
    "            self.inventory += 1\n",
    "            reward -= current_price\n",
    "        elif action == 2:  # Sell\n",
    "            self.inventory -= 1\n",
    "            reward += current_price\n",
    "        \n",
    "        self.current_step += 1\n",
    "        \n",
    "        if self.current_step >= len(self.data) - self.window_size:\n",
    "            done = True\n",
    "        else:\n",
    "            done = False\n",
    "        \n",
    "        obs = self._next_observation()\n",
    "        return obs, reward, done, {}\n",
    "    \n",
    "    def render(self, mode='human', close=False):\n",
    "        pass\n",
    "\n",
    "# Load historical data\n",
    "data = pd.read_csv('historical_data.csv')  \n",
    "\n",
    "# Create the environment with historical data\n",
    "env = HistoricalDataEnv(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the trained model\n",
    "model = A2C.load(\"a2c_mbt_gym_model\")\n",
    "\n",
    "# Reset the environment\n",
    "obs = env.reset()\n",
    "\n",
    "# Run the backtest\n",
    "for _ in range(len(data) - env.window_size):\n",
    "    action, _states = model.predict(obs)\n",
    "    obs, rewards, done, info = env.step(action)\n",
    "    if done:\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Result Analysis & Visualization"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
